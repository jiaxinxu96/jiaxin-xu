<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>

<!-- ======================================================================= -->
<script src="http://www.google.com/jsapi" type="text/javascript"></script>
<script type="text/javascript">google.load("jquery", "1.3.2");</script>
<style type="text/css">
    table {
        margin-bottom: 5px;
        margin-top: 5px;
    }
    body {
        font-family: "Titillium Web", "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;
        font-weight: 300;
        font-size: 18px;
        margin-left: auto;
        margin-right: auto;
        width: 60%;
    }

    h1 {
        font-weight: 300;
    }

    .disclaimerbox {
        background-color: #eee;
        border: 1px solid #eeeeee;
        border-radius: 10px;
        -moz-border-radius: 10px;
        -webkit-border-radius: 10px;
        padding: 20px;
    }

    video.header-vid {
        height: 140px;
        border: 1px solid black;
        border-radius: 10px;
        -moz-border-radius: 10px;
        -webkit-border-radius: 10px;
    }

    img {
        width: 100%
    }

    img.header-img {
        height: 140px;
        border: 1px solid black;
        border-radius: 10px;
        -moz-border-radius: 10px;
        -webkit-border-radius: 10px;
    }

    img.rounded {
        border: 1px solid #eeeeee;
        border-radius: 10px;
        -moz-border-radius: 10px;
        -webkit-border-radius: 10px;
    }

    a:link, a:visited {
        color: #1367a7;
        text-decoration: none;
    }

    a:hover {
        color: #208799;
    }

    td.dl-link {
        height: 160px;
        text-align: center;
        font-size: 22px;
    }

    .layered-paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
        box-shadow: 0px 0px 1px 1px rgba(0, 0, 0, 0.35), /* The top layer shadow */ 5px 5px 0 0px #fff, /* The second layer */ 5px 5px 1px 1px rgba(0, 0, 0, 0.35), /* The second layer shadow */ 10px 10px 0 0px #fff, /* The third layer */ 10px 10px 1px 1px rgba(0, 0, 0, 0.35), /* The third layer shadow */ 15px 15px 0 0px #fff, /* The fourth layer */ 15px 15px 1px 1px rgba(0, 0, 0, 0.35), /* The fourth layer shadow */ 20px 20px 0 0px #fff, /* The fifth layer */ 20px 20px 1px 1px rgba(0, 0, 0, 0.35), /* The fifth layer shadow */ 25px 25px 0 0px #fff, /* The fifth layer */ 25px 25px 1px 1px rgba(0, 0, 0, 0.35); /* The fifth layer shadow */
        margin-left: 10px;
        margin-right: 45px;
    }


    .layered-paper { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
        box-shadow: 0px 0px 1px 1px rgba(0, 0, 0, 0.35), /* The top layer shadow */ 5px 5px 0 0px #fff, /* The second layer */ 5px 5px 1px 1px rgba(0, 0, 0, 0.35), /* The second layer shadow */ 10px 10px 0 0px #fff, /* The third layer */ 10px 10px 1px 1px rgba(0, 0, 0, 0.35); /* The third layer shadow */
        margin-top: 5px;
        margin-left: 10px;
        margin-right: 30px;
        margin-bottom: 5px;
    }

    .vert-cent {
        position: relative;
        top: 50%;
        transform: translateY(-50%);
    }

    hr {
        border: 0;
        height: 1px;
        background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.75), rgba(0, 0, 0, 0));
    }


    #authors td {
        padding-bottom: 5px;
        padding-top: 30px;
    }
</style>


<head>
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-108048997-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-108048997-1');
  </script>

    <title>DETReg: Unsupervised Pretraining with Region Priors for Object Detection</title>
    <meta property="og:title" content="DETReg: Unsupervised Pretraining with Region Priors for Object Detection"/>
</head>

<body>
<br>
<center>
    <span style="font-size:36px">DETReg: Unsupervised Pretraining with Region Priors for Object Detection</span>

    <br>
    <br>

        <span style="font-size:24px">Preprint. Under review.</span>


    <br>
    <br>

    <table align=center>
        <tr>
            <span style="font-size:24px"><a href="http://www.amirbar.net/">Amir Bar</a><sup>1</sup></span> &nbsp;
            <span style="font-size:24px"><a href="https://xinw.ai/">Xin Wang</a><sup>2</sup></span> &nbsp;
            <span style="font-size:24px"><a href="http://vadimkantorov.com/">Vadim Kantorov</a><sup>1</sup></span> &nbsp;
            <span style="font-size:24px"><a href="https://people.eecs.berkeley.edu/~cjrd/">Colorado J Reed</a><sup>2</sup></span> &nbsp;
            <span style="font-size:24px"><a href="https://roeiherz.github.io/">Roei Herzig</a><sup>1</sup></span>
        </tr><br>
        <tr>

            <span style="font-size:24px"><a href="https://chechiklab.biu.ac.il/">Gal Chechik</a><sup>3,4</sup></span> &nbsp;
            <span style="font-size:24px"><a href="https://anna-rohrbach.net/">Anna Rohrbach</a><sup>2</sup></span> &nbsp;
            <span style="font-size:24px"><a href="https://people.eecs.berkeley.edu/~trevor/">Trevor Darrell</a><sup>2</sup></span> &nbsp;
            <span style="font-size:24px"><a href="http://www.cs.tau.ac.il/~gamir/">Amir Globerson</a><sup>1</sup></span> &nbsp;
        </tr>
    </table>


<!--    <br><br><br>-->

<!--    <br><br>-->


    <table align=center>
        <tr>
            <td align=center>
                <center>
                    <span style="font-size:20px">
                        <sup>1</sup>Tel-Aviv University &nbsp;<sup>2</sup>Berkeley AI Research &nbsp;<sup>3</sup>Nvidia &nbsp;<sup>4</sup>Bar-Ilan University
                    </span>
                </center>
            </td>
        </tr>
    </table>
<!--    <br>-->


<!--    <span style="font-size:24px">Preprint. Under review.</span>-->



</center>
<!--<left>    <span style="font-size:18px"><sup>*</sup>Equally contributed</span> </left>-->
<center><img src="data/illustration.png" align="middle"></center>
<br>
    <table align=center>
        <tr>
            <td align=center>
                <center>
                    <span style="font-size:24px"><a href="https://arxiv.org/abs/2106.04550"> [Paper]</a></span>
                </center>
            </td>
            <td align=center>
                <center>
                        <span style="font-size:24px"><a href='https://github.com/amirbar/DETReg'> [GitHub]</a></span>
                </center>
            </td>
            <td align=center>
                <center>
                        <span style="font-size:24px"><a href='data/bib.txt'> [Bibtex]</a></span>
                </center>
            </td>
        </tr>
    </table>

<br><br>
<hr>
<table align=center>
    <center><h1>Abstract</h1></center>
</table>

We present DETReg, an unsupervised pretraining approach for object <b>DE</b>tection with <b>TR</b>ansformers using <b>Reg</b>ion priors.
Motivated by the two tasks underlying object detection: <b>localization</b> and <b>categorization</b>, we combine two complementary signals for self-supervision. For an object localization signal, we use pseudo ground truth object bounding boxes from an off-the-shelf unsupervised region proposal method, Selective Search, which does not require training data and can detect objects at a high recall rate and very low precision.
The categorization signal comes from an object embedding loss that encourages invariant object representations, from which the object category can be inferred.
We show how to combine these two signals to train the Deformable DETR detection architecture from large amounts of unlabeled data. DETReg improves the performance over competitive baselines and previous self-supervised methods on standard benchmarks like MS COCO and PASCAL VOC. DETReg also outperforms previous supervised and unsupervised baseline approaches on low-data regime when trained with only 1%, 2%, 5%, and 10% of the labeled data on MS COCO.

<br>
<hr>

<table align=center>
    <center><h1 id="model">DETReg Pretext task</h1></center>
</table>
<p>
    Our approach to the problem is based on the observation that learning good detectors requires learning to detect objects in the pretraining stage. To accomplish this, we present a new framework called ``DEtection with TRansformers based on Region priors'', or <b>DETReg</b>.
    DETReg can be used to train a detector on unlabeled data by introducing two key pretraining tasks: ``Object Localization Task'' and the ``Object Embedding Task''. The goal of the first is to train the model to localize objects, regardless of their categories. However, learning to localize objects is not enough, and detectors must also classify objects. Towards this end, we introduce the ``Object Embedding Task'', which is geared towards understanding the <b>categories</b> of objects in the image. Inspired by the simplicity of recent transformers for object detection, we choose to base our approach on the Deformable DETR architecture, which simplifies the implementation and is very fast to train.</p>
<center><img src="/detreg/data/model_figure_v2.png" align="middle"></center>
<table align=center>
    <center><h1 id="viz">Visualization</h1></center>
</table>
<p>
    Qualitative examples of DETReg unsupervised box predictions. This shows the pixel-level gradient norm for the x/y bounding box center and the object embedding. These gradient norms indicate how sensitive the predicted values are to perturbations of the input pixels. For the first three columns, DETReg attends to the object edges for the x/y predictions and z for the predicted object embedding. The final column shows a limitation where the space surrounding the object is used for the embedding.
</p>
<center><img src="/detreg/data/viz.png" align="middle"></center>

<br><br>
<hr>
<br>
<table align=center>
    <center><h1>Paper</h1></center>
    <tr>
        <td><a href="https://arxiv.org/abs/2106.04550"><img class="layered-paper-big" style="height:175px; width: 150px; margin-bottom: 50px"
                                                                src="/detreg/data/paper.png"/></a></td>
        <td><a href="https://arxiv.org/abs/2106.04550"></a></td>
        <td>
        <span style="font-size:14pt"> <i>DETReg: Unsupervised Pretraining with Region Priors for Object Detection</i><br>Amir Bar, Xin Wang, Vadim Kantorov, Colorado J Reed, Roei Herzig, <br>Gal Chechik, Anna Rohrbach, Trevor Darrell, Amir Globerson<br>

            Arxiv<br>
            Hosted on <a href="https://arxiv.org/abs/2106.04550">arXiv</a>
                </span>
            <br>
        </td>
        <span style="font-size:4pt"><a href="https://arxiv.org/abs/2106.04550"><br></a></span>
    </tr>
</table>


<hr>
<table align=center style="margin-bottom: 50px">
    <tr>
        <td>
            <left>
                <center><h1>Acknowledgements</h1></center>

We would like to thank Sayna Ebrahimi for helpful feedback and discussions. This project has received funding from the European Research Council (ERC) under the European Unions Horizon 2020 research and innovation programme (grant ERC HOLI 819080). Prof. Darrellâ€™s group was supported in part by DoD including DARPA's XAI, LwLL, and/or SemaFor programs, as well as BAIR's industrial alliance programs. GC group was supported by the Israel Science Foundation (ISF 737/2018), and by an equipment grant to GC and Bar-Ilan University from the Israel Science Foundation (ISF 2332/18). This work was completed in partial fulfillment for the Ph.D degree of the first author.
</left>
        </td>
    </tr>
</table>


</body>
</html>
